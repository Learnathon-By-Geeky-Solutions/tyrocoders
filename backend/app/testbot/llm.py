"""
llm_interface.py

This module handles interactions with the Gemini LLM (Language Model) API. It includes
functions to construct prompts using contextual documents, send these prompts to the 
LLM, and return the model's responses.

Functions:
- create_prompt: Builds a structured prompt string using a question and a list of context documents.
- ask_llm: Sends the structured prompt to the Gemini API and retrieves a response.
- process_query: Orchestrates the end-to-end process of constructing a prompt and querying the LLM.

Logging is used for tracking API calls and error handling.
"""

import os
import json
import requests
import logging
from typing import Dict, Any, Optional
from config import GEMINI_API_URL

# Configure logging
logger = logging.getLogger(__name__)

def create_prompt(query: str, context_docs: list) -> str:
    """
    Create a structured prompt for the LLM based on the query and context documents.

    Args:
        query (str): The user's input question.
        context_docs (list): A list of text documents providing contextual information.

    Returns:
        str: A formatted prompt string to be passed to the LLM.
    """
    context_text = "\n\n".join([f"Document {i+1}:\n{doc}" for i, doc in enumerate(context_docs)])
    
    prompt = f"""You are a helpful AI assistant providing accurate answers based on the provided context. 
If the answer isn't found in the context, politely say you don't know instead of making up information.

## CONTEXT:
{context_text}

## QUESTION:
{query}

## ANSWER:
"""
    return prompt

def ask_llm(prompt: str) -> str:
    """
    Send a structured prompt to the Gemini API and return the model's response.

    Args:
        prompt (str): The input string to be processed by the LLM.

    Returns:
        str: The text response from the model, or an error message if the request fails.
    """
    try:
        payload = {
            "contents": [
                {
                    "parts": [
                        {"text": prompt}
                    ]
                }
            ],
            "generationConfig": {
                "temperature": 0.2,
                "maxOutputTokens": 1024,
                "topP": 0.95,
                "topK": 40
            }
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload))
        
        if response.status_code == 200:
            response_data = response.json()
            text = response_data.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
            return text
        else:
            logger.error(f"API request failed with status {response.status_code}: {response.text}")
            return f"Error: Failed to get response from LLM (Status {response.status_code})"
    
    except Exception as e:
        logger.error(f"Error in LLM request: {e}")
        return f"Error: {str(e)}"

def process_query(chatbot_id: str, query: str, context_docs: list) -> str:
    """
    Process a user query using the appropriate chatbot context and return the LLM's response.

    Args:
        chatbot_id (str): Identifier for the chatbot instance.
        query (str): The question posed by the user.
        context_docs (list): List of relevant documents to provide context for the LLM.

    Returns:
        str: The response generated by the LLM based on the given context and query.
    """
    prompt = create_prompt(query, context_docs)
    logger.info(f"Sending prompt to LLM for chatbot {chatbot_id}")
    response = ask_llm(prompt)
    return response
